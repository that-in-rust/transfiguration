//! Core data types for parallel ONNX LLM inference system
//!
//! Follows TDD-First architecture with executable contracts and measurable performance.

use std::time::Duration;
use serde::{Deserialize, Serialize};
use uuid::Uuid;

/// Represents a processing result from a single ONNX model instance
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingResult {
    /// Unique identifier for this result
    pub id: Uuid,
    /// ID of the chunk that was processed
    pub chunk_id: Uuid,
    /// The 1-line summary generated by the model
    pub summary: String,
    /// Time taken to generate this summary
    pub processing_time: Duration,
    /// Model instance that generated this result
    pub model_instance_id: u32,
    /// Confidence score from the model (0.0-1.0)
    pub confidence: f32,
}

/// Performance contracts for ONNX model operations
#[derive(Debug, Clone)]
pub struct ModelPerformanceContract {
    /// Maximum time allowed per chunk processing (seconds)
    pub max_processing_time_s: f32,
    /// Minimum confidence score for valid summaries
    pub min_confidence: f32,
    /// Maximum summary length in characters
    pub max_summary_length: usize,
    /// Target parallel instances
    pub target_parallel_instances: usize,
}

impl Default for ModelPerformanceContract {
    fn default() -> Self {
        Self {
            max_processing_time_s: 5.0, // 5 seconds per chunk max
            min_confidence: 0.6, // 60% confidence minimum
            max_summary_length: 120, // 1 line ~ 120 chars max
            target_parallel_instances: 20, // 20 parallel instances
        }
    }
}

/// Processing statistics for monitoring and validation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessingStats {
    /// Total chunks processed
    pub total_chunks: usize,
    /// Successfully processed chunks
    pub successful_chunks: usize,
    /// Failed chunks
    pub failed_chunks: usize,
    /// Average processing time per chunk
    pub avg_processing_time: Duration,
    /// Total processing time for all chunks
    pub total_processing_time: Duration,
    /// Parallel efficiency (actual_time / theoretical_optimal_time)
    pub parallel_efficiency: f32,
    /// Model instances utilized
    pub instances_utilized: usize,
}

/// Error types for ONNX processing operations
#[derive(thiserror::Error, Debug)]
pub enum ProcessingError {
    #[error("Model initialization failed for instance {instance_id}: {message}")]
    ModelInitializationFailed {
        instance_id: u32,
        message: String,
    },

    #[error("Model inference failed for instance {instance_id}: {message}")]
    InferenceFailed {
        instance_id: u32,
        message: String,
    },

    #[error("Parallel processing coordination failed: {message}")]
    ParallelCoordinationFailed { message: String },

    #[error("Performance contract violation: {constraint} = {actual}, expected: {expected}")]
    PerformanceViolation {
        constraint: String,
        actual: String,
        expected: String,
    },

    #[error("Invalid chunk for processing: {reason}")]
    InvalidChunk { reason: String },

    #[error("Resource exhaustion: {resource} limit {limit} reached")]
    ResourceExhaustion {
        resource: String,
        limit: usize,
    },
}

/// Contract validation result for performance monitoring
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContractValidation {
    /// Whether the contract was satisfied
    pub satisfied: bool,
    /// Performance metric being validated
    pub metric: String,
    /// Actual value observed
    pub actual: String,
    /// Expected value from contract
    pub expected: String,
    /// Human-readable validation message
    pub message: String,
}

impl ContractValidation {
    /// Create successful validation
    pub fn success(metric: &str, actual: &str, expected: &str) -> Self {
        Self {
            satisfied: true,
            metric: metric.to_string(),
            actual: actual.to_string(),
            expected: expected.to_string(),
            message: format!("✅ {}: {} (within {})", metric, actual, expected),
        }
    }

    /// Create failed validation
    pub fn failure(metric: &str, actual: &str, expected: &str) -> Self {
        Self {
            satisfied: false,
            metric: metric.to_string(),
            actual: actual.to_string(),
            expected: expected.to_string(),
            message: format!("❌ {}: {} (expected ≤ {})", metric, actual, expected),
        }
    }
}

/// Trait definition for ONNX model providers to enable dependency injection
pub trait ModelProvider: Send + Sync {
    /// Initialize model instance
    async fn initialize(&self, instance_id: u32) -> Result<(), ProcessingError>;

    /// Process chunk to generate 1-line summary
    async fn process_chunk(
        &self,
        chunk_content: &str,
        instance_id: u32,
    ) -> Result<ProcessingResult, ProcessingError>;

    /// Cleanup model resources
    async fn cleanup(&self, instance_id: u32) -> Result<(), ProcessingError>;

    /// Get model metadata
    fn model_info(&self) -> ModelInfo;
}

/// Model metadata for capability validation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ModelInfo {
    pub name: String,
    pub version: String,
    pub max_sequence_length: usize,
    pub supports_parallel: bool,
    pub memory_requirement_mb: usize,
}

/// Batch processing configuration with TDD contracts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchConfig {
    /// Maximum chunks to process in parallel
    pub max_parallel_chunks: usize,
    /// Timeout for individual chunk processing
    pub chunk_timeout: Duration,
    /// Overall batch processing timeout
    pub batch_timeout: Duration,
    /// Whether to continue processing on individual failures
    pub continue_on_failure: bool,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_parallel_chunks: 20, // Match MVP requirement
            chunk_timeout: Duration::from_secs(5),
            batch_timeout: Duration::from_secs(300), // 5 minutes max
            continue_on_failure: true, // Don't stop all processing for individual failures
        }
    }
}

/// Processing state for tracking batch operations
#[derive(Debug, Clone)]
pub enum ProcessingState {
    Initialized,
    LoadingChunks,
    ProcessingParallel,
    CollectingResults,
    Completed,
    Failed(String),
}

/// Main orchestrator state for monitoring system health
#[derive(Debug, Clone)]
pub struct OrchestratorState {
    pub current_state: ProcessingState,
    pub chunks_processed: usize,
    pub total_chunks: usize,
    pub active_model_instances: usize,
    pub results_collected: usize,
    pub start_time: std::time::Instant,
}

impl OrchestratorState {
    pub fn new(total_chunks: usize) -> Self {
        Self {
            current_state: ProcessingState::Initialized,
            chunks_processed: 0,
            total_chunks,
            active_model_instances: 0,
            results_collected: 0,
            start_time: std::time::Instant::now(),
        }
    }

    /// Check if all chunks have been processed
    pub fn is_complete(&self) -> bool {
        self.results_collected >= self.total_chunks
    }

    /// Get processing progress as percentage
    pub fn progress(&self) -> f32 {
        if self.total_chunks == 0 {
            return 0.0;
        }
        (self.results_collected as f32 / self.total_chunks as f32) * 100.0
    }

    /// Get elapsed time since processing started
    pub fn elapsed(&self) -> Duration {
        self.start_time.elapsed()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_performance_contract_default() {
        let contract = ModelPerformanceContract::default();
        assert_eq!(contract.max_processing_time_s, 5.0);
        assert_eq!(contract.min_confidence, 0.6);
        assert_eq!(contract.max_summary_length, 120);
        assert_eq!(contract.target_parallel_instances, 20);
    }

    #[test]
    fn test_batch_config_default() {
        let config = BatchConfig::default();
        assert_eq!(config.max_parallel_chunks, 20);
        assert_eq!(config.chunk_timeout, Duration::from_secs(5));
        assert_eq!(config.batch_timeout, Duration::from_secs(300));
        assert!(config.continue_on_failure);
    }

    #[test]
    fn test_orchestrator_state_progress() {
        let state = OrchestratorState::new(100);
        assert_eq!(state.progress(), 0.0);
        assert!(!state.is_complete());

        // Simulate progress
        let mut updated_state = state.clone();
        updated_state.results_collected = 50;
        assert_eq!(updated_state.progress(), 50.0);
        assert!(!updated_state.is_complete());

        // Complete state
        updated_state.results_collected = 100;
        assert_eq!(updated_state.progress(), 100.0);
        assert!(updated_state.is_complete());
    }

    #[test]
    fn test_contract_validation_creation() {
        let success = ContractValidation::success("processing_time", "2.3s", "5.0s");
        assert!(success.satisfied);
        assert!(success.message.starts_with("✅"));

        let failure = ContractValidation::failure("processing_time", "7.2s", "5.0s");
        assert!(!failure.satisfied);
        assert!(failure.message.starts_with("❌"));
    }

    #[test]
    fn test_processing_result_serialization() {
        let result = ProcessingResult {
            id: Uuid::new_v4(),
            chunk_id: Uuid::new_v4(),
            summary: "Test summary".to_string(),
            processing_time: Duration::from_millis(1500),
            model_instance_id: 5,
            confidence: 0.85,
        };

        // Test serialization/deserialization
        let json = serde_json::to_string(&result).unwrap();
        let deserialized: ProcessingResult = serde_json::from_str(&json).unwrap();

        assert_eq!(result.summary, deserialized.summary);
        assert_eq!(result.confidence, deserialized.confidence);
        assert_eq!(result.model_instance_id, deserialized.model_instance_id);
    }
}