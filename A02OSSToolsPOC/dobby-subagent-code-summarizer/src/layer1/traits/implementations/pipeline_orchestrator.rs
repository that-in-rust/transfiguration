//! PipelineOrchestrator: Combining DatabaseProvider and InferenceEngine
//!
//! Provides high-level orchestration for the complete code summarization pipeline
//! Follows Rust async patterns with proper error handling and resource management

use crate::layer1::traits::inference::*;
use crate::layer1::traits::database::*;
use crate::layer1::traits::error::*;
use crate::layer1::traits::implementations::inference_engine::TraitInferenceEngine;
use async_trait::async_trait;
use std::sync::Arc;
use std::time::{Duration, Instant};
use futures::StreamExt;
use tokio::sync::Semaphore;
use uuid::Uuid;

/// Pipeline orchestrator that combines database and inference operations
pub struct PipelineOrchestrator {
    inference_engine: Arc<dyn InferenceEngine<Output = InferenceResult, Error = InferenceError>>,
    database_provider: Arc<dyn DatabaseProvider<Error = DatabaseError>>,
    config: PipelineConfig,
    semaphore: Arc<Semaphore>,
}

/// Pipeline configuration with performance and resource limits
#[derive(Debug, Clone)]
pub struct PipelineConfig {
    pub max_concurrent_jobs: usize,
    pub chunk_size: usize,
    pub max_processing_time: Duration,
    pub retry_config: RetryConfig,
    pub caching_config: CachingConfig,
    pub monitoring_config: MonitoringConfig,
}

#[derive(Debug, Clone)]
pub struct RetryConfig {
    pub max_retries: usize,
    pub base_delay: Duration,
    pub max_delay: Duration,
    pub backoff_multiplier: f64,
}

#[derive(Debug, Clone)]
pub struct CachingConfig {
    pub enable_caching: bool,
    pub cache_ttl: Duration,
    pub max_cache_size: usize,
}

#[derive(Debug, Clone)]
pub struct MonitoringConfig {
    pub enable_metrics: bool,
    pub metrics_interval: Duration,
    pub enable_tracing: bool,
}

impl Default for PipelineConfig {
    fn default() -> Self {
        Self {
            max_concurrent_jobs: 10,
            chunk_size: 1000,
            max_processing_time: Duration::from_secs(300), // 5 minutes
            retry_config: RetryConfig {
                max_retries: 3,
                base_delay: Duration::from_millis(100),
                max_delay: Duration::from_secs(10),
                backoff_multiplier: 2.0,
            },
            caching_config: CachingConfig {
                enable_caching: true,
                cache_ttl: Duration::from_secs(3600), // 1 hour
                max_cache_size: 10000,
            },
            monitoring_config: MonitoringConfig {
                enable_metrics: true,
                metrics_interval: Duration::from_secs(30),
                enable_tracing: true,
            },
        }
    }
}

/// Pipeline job representing a code summarization task
#[derive(Debug, Clone)]
pub struct PipelineJob {
    pub job_id: JobId,
    pub input: JobInput,
    pub options: JobOptions,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Clone)]
pub struct JobId(pub Uuid);

#[derive(Debug, Clone)]
pub enum JobInput {
    File { path: String, content: String },
    Repository { url: String, files: Vec<String> },
    Text { content: String, filename: Option<String> },
}

#[derive(Debug, Clone)]
pub struct JobOptions {
    pub chunking_strategy: ChunkingStrategy,
    pub inference_config: InferenceParams,
    pub output_format: OutputFormat,
    pub include_metadata: bool,
}

#[derive(Debug, Clone)]
pub enum ChunkingStrategy {
    Lines { max_lines: usize },
    Characters { max_chars: usize },
    Semantic { max_tokens: usize },
}

#[derive(Debug, Clone)]
pub enum OutputFormat {
    PlainText,
    Markdown,
    Json,
    Structured,
}

/// Pipeline job result with comprehensive metadata
#[derive(Debug, Clone)]
pub struct PipelineJobResult {
    pub job_id: JobId,
    pub status: JobStatus,
    pub chunks_processed: usize,
    pub summaries: Vec<ChunkSummary>,
    pub processing_time: Duration,
    pub error: Option<PipelineError>,
    pub metadata: JobMetadata,
}

#[derive(Debug, Clone)]
pub enum JobStatus {
    Pending,
    InProgress { progress: f64 },
    Completed,
    Failed { error: PipelineError },
    Cancelled,
}

#[derive(Debug, Clone)]
pub struct ChunkSummary {
    pub chunk_id: String,
    pub summary: String,
    pub confidence: f64,
    pub token_count: usize,
    pub processing_time: Duration,
    pub metadata: ChunkMetadata,
}

#[derive(Debug, Clone)]
pub struct ChunkMetadata {
    pub start_line: Option<usize>,
    pub end_line: Option<usize>,
    pub file_path: Option<String>,
    pub chunk_type: String,
}

#[derive(Debug, Clone)]
pub struct JobMetadata {
    pub created_at: chrono::DateTime<chrono::Utc>,
    pub started_at: Option<chrono::DateTime<chrono::Utc>>,
    pub completed_at: Option<chrono::DateTime<chrono::Utc>>,
    pub worker_id: Option<String>,
    pub performance_metrics: PerformanceMetrics,
}

#[derive(Debug, Clone)]
pub struct PerformanceMetrics {
    pub total_chunks: usize,
    pub successful_chunks: usize,
    pub failed_chunks: usize,
    pub average_chunk_time: Duration,
    pub total_inference_time: Duration,
    pub total_database_time: Duration,
    pub memory_peak_mb: usize,
}

impl PipelineOrchestrator {
    /// Create new pipeline orchestrator
    pub fn new(
        inference_engine: Arc<dyn InferenceEngine<Output = InferenceResult, Error = InferenceError>>,
        database_provider: Arc<dyn DatabaseProvider<Error = DatabaseError>>,
        config: PipelineConfig,
    ) -> Self {
        Self {
            inference_engine,
            database_provider,
            semaphore: Arc::new(Semaphore::new(config.max_concurrent_jobs)),
            config,
        }
    }

    /// Process a single job
    pub async fn process_job(&self, job: PipelineJob) -> Result<PipelineJobResult, PipelineError> {
        let start_time = Instant::now();
        let job_id = job.job_id.clone();

        // Acquire semaphore permit for concurrency control
        let _permit = self.semaphore.acquire().await
            .map_err(|_| PipelineError::ConcurrencyLimitExceeded {
                current: self.config.max_concurrent_jobs,
                max: self.config.max_concurrent_jobs,
            })?;

        let mut metadata = JobMetadata {
            created_at: job.created_at,
            started_at: Some(chrono::Utc::now()),
            completed_at: None,
            worker_id: Some(format!("worker-{}", uuid::Uuid::new_v4())),
            performance_metrics: PerformanceMetrics {
                total_chunks: 0,
                successful_chunks: 0,
                failed_chunks: 0,
                average_chunk_time: Duration::ZERO,
                total_inference_time: Duration::ZERO,
                total_database_time: Duration::ZERO,
                memory_peak_mb: 0,
            },
        };

        // Extract content from job input
        let content = match &job.input {
            JobInput::File { path, content } => {
                // Store file metadata in database
                self.store_file_metadata(&job_id, path, content.len()).await?;
                content.clone()
            }
            JobInput::Repository { url, files } => {
                // Process repository files
                self.process_repository_files(&job_id, url, files).await?
            }
            JobInput::Text { content, filename } => {
                // Store text content
                if let Some(filename) = filename {
                    self.store_text_content(&job_id, filename, content).await?;
                }
                content.clone()
            }
        };

        // Chunk the content
        let chunks = self.chunk_content(&content, &job.options.chunking_strategy)
            .map_err(|e| PipelineError::Configuration {
                section: "chunking".to_string(),
                field: "strategy".to_string(),
                value: format!("{:?}", job.options.chunking_strategy),
            })?;

        metadata.performance_metrics.total_chunks = chunks.len();

        // Process chunks with batch inference
        let summaries = self.process_chunks_with_retry(&chunks, &job.options).await?;

        // Update performance metrics
        let successful_chunks = summaries.len();
        metadata.performance_metrics.successful_chunks = successful_chunks;
        metadata.performance_metrics.failed_chunks = chunks.len() - successful_chunks;

        if successful_chunks > 0 {
            let total_time: Duration = summaries.iter().map(|s| s.processing_time).sum();
            metadata.performance_metrics.average_chunk_time = total_time / successful_chunks as u32;
        }

        // Store results in database
        self.store_job_results(&job_id, &summaries, &metadata).await?;

        let processing_time = start_time.elapsed();
        metadata.completed_at = Some(chrono::Utc::now());

        // Validate performance contracts
        if processing_time > self.config.max_processing_time {
            return Err(PipelineError::PerformanceContract {
                operation: "job_processing".to_string(),
                actual: processing_time,
                expected: self.config.max_processing_time,
            });
        }

        Ok(PipelineJobResult {
            job_id,
            status: JobStatus::Completed,
            chunks_processed: successful_chunks,
            summaries,
            processing_time,
            error: None,
            metadata,
        })
    }

    /// Process multiple jobs concurrently
    pub async fn process_batch_jobs(&self, jobs: Vec<PipelineJob>) -> Vec<Result<PipelineJobResult, PipelineError>> {
        let mut handles = Vec::new();

        for job in jobs {
            let orchestrator = self.clone();
            let handle = tokio::spawn(async move {
                orchestrator.process_job(job).await
            });
            handles.push(handle);
        }

        let mut results = Vec::new();
        for handle in handles {
            match handle.await {
                Ok(result) => results.push(result),
                Err(e) => results.push(Err(PipelineError::System {
                    message: format!("Job processing task panicked: {}", e),
                })),
            }
        }

        results
    }

    /// Stream processing for large inputs
    pub async fn process_stream_job(
        &self,
        job: PipelineJob,
        content_stream: impl futures::Stream<Item = String> + Send,
    ) -> Result<impl futures::Stream<Item = Result<ChunkSummary, PipelineError>> + Send, PipelineError> {
        let (tx, rx) = tokio::sync::mpsc::channel(100);
        let orchestrator = self.clone();

        // Spawn streaming task
        tokio::spawn(async move {
            let mut stream = Box::pin(content_stream);
            let mut chunk_buffer = String::new();
            let mut line_count = 0;

            loop {
                match stream.next().await {
                    Some(content_line) => {
                        chunk_buffer.push_str(&content_line);
                        chunk_buffer.push('\n');
                        line_count += 1;

                        // Process chunk when it reaches the configured size
                        if line_count >= orchestrator.config.chunk_size {
                            let chunk_summary = orchestrator.process_single_chunk(
                                &chunk_buffer,
                                &job.options,
                                0,
                                line_count,
                                None,
                            ).await;

                            if tx.send(chunk_summary).await.is_err() {
                                break; // Channel closed
                            }

                            chunk_buffer.clear();
                            line_count = 0;
                        }
                    }
                    None => {
                        // Process remaining content
                        if !chunk_buffer.is_empty() {
                            let chunk_summary = orchestrator.process_single_chunk(
                                &chunk_buffer,
                                &job.options,
                                0,
                                line_count,
                                None,
                            ).await;

                            let _ = tx.send(chunk_summary).await;
                        }
                        break;
                    }
                }
            }
        });

        Ok(tokio_stream::wrappers::ReceiverStream::new(rx))
    }

    /// Get pipeline health and status
    pub async fn health_check(&self) -> Result<PipelineHealth, PipelineError> {
        let inference_health = self.inference_engine.health_check().await
            .map_err(|e| PipelineError::Inference {
                model: "unknown".to_string(),
                message: "Inference engine health check failed".to_string(),
                source: e,
            })?;

        let database_health = self.database_provider.health_check().await
            .map_err(|e| PipelineError::Database {
                operation: "health_check".to_string(),
                table: "system".to_string(),
                source: e,
            })?;

        let available_permits = self.semaphore.available_permits();
        let utilization_rate = (self.config.max_concurrent_jobs - available_permits) as f64 / self.config.max_concurrent_jobs as f64;

        Ok(PipelineHealth {
            inference_health,
            database_health,
            available_jobs: available_permits,
            utilization_rate,
            status: if utilization_rate < 0.9 {
                PipelineStatus::Healthy
            } else if utilization_rate < 1.0 {
                PipelineStatus::Degraded
            } else {
                PipelineStatus::Overloaded
            },
        })
    }

    /// Get pipeline metrics
    pub async fn get_metrics(&self) -> Result<PipelineMetrics, PipelineError> {
        let session_info = self.inference_engine.session_info().await
            .map_err(|e| PipelineError::Inference {
                model: "unknown".to_string(),
                message: "Failed to get session info".to_string(),
                source: e,
            })?;

        Ok(PipelineMetrics {
            active_jobs: self.config.max_concurrent_jobs - self.semaphore.available_permits(),
            max_concurrent_jobs: self.config.max_concurrent_jobs,
            inference_sessions: session_info,
            database_connections: 0, // Would get from database provider
            memory_usage_mb: self.get_current_memory_usage(),
            uptime: self.get_uptime(),
        })
    }

    // Private helper methods

    fn chunk_content(&self, content: &str, strategy: &ChunkingStrategy) -> Result<Vec<String>, PipelineError> {
        match strategy {
            ChunkingStrategy::Lines { max_lines } => {
                let lines: Vec<&str> = content.lines().collect();
                let mut chunks = Vec::new();

                for chunk in lines.chunks(*max_lines) {
                    chunks.push(chunk.join("\n"));
                }

                Ok(chunks)
            }
            ChunkingStrategy::Characters { max_chars } => {
                let mut chunks = Vec::new();
                for chunk in content.as_bytes().chunks(*max_chars) {
                    chunks.push(String::from_utf8_lossy(chunk).to_string());
                }
                Ok(chunks)
            }
            ChunkingStrategy::Semantic { max_tokens } => {
                // Simplified semantic chunking - in practice would use sentence boundaries
                let estimated_chars_per_token = 4;
                let max_chars = max_tokens * estimated_chars_per_token;
                self.chunk_content(content, &ChunkingStrategy::Characters { max_chars })
            }
        }
    }

    async fn process_chunks_with_retry(
        &self,
        chunks: &[String],
        options: &JobOptions,
    ) -> Result<Vec<ChunkSummary>, PipelineError> {
        let mut summaries = Vec::new();
        let mut failed_chunks = 0;

        for (i, chunk) in chunks.iter().enumerate() {
            match self.process_single_chunk_with_retry(chunk, options, i, chunks.len(), None).await {
                Ok(summary) => summaries.push(summary),
                Err(e) => {
                    failed_chunks += 1;
                    if failed_chunks > chunks.len() / 2 {
                        return Err(PipelineError::ResourceExhaustion {
                            resource: "chunk_processing".to_string(),
                            used: summaries.len(),
                            limit: chunks.len(),
                        });
                    }
                    // Continue processing other chunks
                }
            }
        }

        Ok(summaries)
    }

    async fn process_single_chunk_with_retry(
        &self,
        chunk: &str,
        options: &JobOptions,
        chunk_index: usize,
        total_chunks: usize,
        file_path: Option<String>,
    ) -> Result<ChunkSummary, PipelineError> {
        let mut last_error = None;

        for attempt in 0..=self.config.retry_config.max_retries {
            match self.process_single_chunk(chunk, options, chunk_index, total_chunks, file_path.clone()).await {
                Ok(summary) => return Ok(summary),
                Err(e) => {
                    last_error = Some(e);
                    if attempt < self.config.retry_config.max_retries {
                        let delay = self.calculate_retry_delay(attempt);
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| PipelineError::System {
            message: "Chunk processing failed with unknown error".to_string(),
        }))
    }

    async fn process_single_chunk(
        &self,
        chunk: &str,
        options: &JobOptions,
        chunk_index: usize,
        total_chunks: usize,
        file_path: Option<String>,
    ) -> Result<ChunkSummary, PipelineError> {
        let start_time = Instant::now();

        // Perform inference
        let inference_result = self.inference_engine.infer(chunk.to_string()).await
            .map_err(|e| PipelineError::Inference {
                model: "default".to_string(),
                message: format!("Failed to process chunk {}: {}", chunk_index, e),
                source: e,
            })?;

        let processing_time = start_time.elapsed();

        Ok(ChunkSummary {
            chunk_id: format!("chunk-{}", chunk_index),
            summary: inference_result.content,
            confidence: inference_result.confidence,
            token_count: inference_result.token_count,
            processing_time,
            metadata: ChunkMetadata {
                start_line: Some(chunk_index * 100), // Approximate
                end_line: Some((chunk_index + 1) * 100),
                file_path,
                chunk_type: "code".to_string(),
            },
        })
    }

    fn calculate_retry_delay(&self, attempt: usize) -> Duration {
        let delay = self.config.retry_config.base_delay * self.config.retry_config.backoff_multiplier.powi(attempt as i32);
        std::cmp::min(delay, self.config.retry_config.max_delay)
    }

    async fn store_file_metadata(&self, job_id: &JobId, path: &str, size: usize) -> Result<(), PipelineError> {
        // Implementation would store file metadata in database
        // For now, just return success
        Ok(())
    }

    async fn process_repository_files(&self, job_id: &JobId, url: &str, files: &[String]) -> Result<String, PipelineError> {
        // Implementation would fetch and concatenate repository files
        // For now, return placeholder content
        Ok(format!("// Repository: {}\n// Files: {:?}\n// Content would be fetched here", url, files))
    }

    async fn store_text_content(&self, job_id: &JobId, filename: &str, content: &str) -> Result<(), PipelineError> {
        // Implementation would store text content in database
        Ok(())
    }

    async fn store_job_results(&self, job_id: &JobId, summaries: &[ChunkSummary], metadata: &JobMetadata) -> Result<(), PipelineError> {
        // Implementation would store job results in database
        Ok(())
    }

    fn get_current_memory_usage(&self) -> usize {
        memory_stats::memory_stats()
            .map(|stats| stats.physical_mem / 1024 / 1024)
            .unwrap_or(0)
    }

    fn get_uptime(&self) -> Duration {
        // Would track orchestrator start time
        Duration::from_secs(0)
    }
}

impl Clone for PipelineOrchestrator {
    fn clone(&self) -> Self {
        Self {
            inference_engine: Arc::clone(&self.inference_engine),
            database_provider: Arc::clone(&self.database_provider),
            config: self.config.clone(),
            semaphore: Arc::clone(&self.semaphore),
        }
    }
}

/// Pipeline health status
#[derive(Debug, Clone)]
pub struct PipelineHealth {
    pub inference_health: ModelHealth,
    pub database_health: DatabaseHealth,
    pub available_jobs: usize,
    pub utilization_rate: f64,
    pub status: PipelineStatus,
}

#[derive(Debug, Clone)]
pub enum PipelineStatus {
    Healthy,
    Degraded,
    Overloaded,
    Unhealthy,
}

/// Pipeline metrics
#[derive(Debug, Clone)]
pub struct PipelineMetrics {
    pub active_jobs: usize,
    pub max_concurrent_jobs: usize,
    pub inference_sessions: SessionInfo,
    pub database_connections: usize,
    pub memory_usage_mb: usize,
    pub uptime: Duration,
}

#[async_trait]
pub trait PipelineOrchestratorTrait: Send + Sync + 'static {
    type Error: std::error::Error + Send + Sync + 'static;

    async fn process_job(&self, job: PipelineJob) -> Result<PipelineJobResult, Self::Error>;
    async fn process_batch_jobs(&self, jobs: Vec<PipelineJob>) -> Vec<Result<PipelineJobResult, Self::Error>>;
    async fn health_check(&self) -> Result<PipelineHealth, Self::Error>;
    async fn get_metrics(&self) -> Result<PipelineMetrics, Self::Error>;
}

#[async_trait]
impl PipelineOrchestratorTrait for PipelineOrchestrator {
    type Error = PipelineError;

    async fn process_job(&self, job: PipelineJob) -> Result<PipelineJobResult, Self::Error> {
        self.process_job(job).await
    }

    async fn process_batch_jobs(&self, jobs: Vec<PipelineJob>) -> Vec<Result<PipelineJobResult, Self::Error>> {
        self.process_batch_jobs(jobs).await
    }

    async fn health_check(&self) -> Result<PipelineHealth, Self::Error> {
        self.health_check().await
    }

    async fn get_metrics(&self) -> Result<PipelineMetrics, Self::Error> {
        self.get_metrics().await
    }
}