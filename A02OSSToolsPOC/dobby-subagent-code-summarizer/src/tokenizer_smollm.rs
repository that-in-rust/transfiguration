//! SmolLM2 Tokenizer Implementation
//!
//! Following proven CodeT5 pattern - custom tokenizer with JSON vocabulary

use crate::errors::{ProcessingError, Result};
use std::collections::HashMap;
use std::fs::File;
use std::io::{BufReader, Read};
use std::path::Path;

/// SmolLM2 tokenizer following CodeT5 pattern
#[derive(Debug)]
pub struct SmolLM2Tokenizer {
    vocab: HashMap<String, u32>,
    reverse_vocab: HashMap<u32, String>,
    pad_token_id: u32,
    eos_token_id: u32,
    bos_token_id: u32,
    unk_token_id: u32,
    max_length: usize,
}

impl SmolLM2Tokenizer {
    /// Create SmolLM2 tokenizer from model directory
    ///
    /// # Preconditions
    /// - tokenizer.json file exists in model directory
    /// - File contains valid SmolLM2 tokenizer configuration
    ///
    /// # Postconditions
    /// - Returns Ok(SmolLM2Tokenizer) if tokenizer loads successfully
    /// - Tokenizer ready for encoding/decoding operations
    ///
    /// # Error Conditions
    /// - ProcessingError::TokenizerLoadFailed if files don't exist
    /// - ProcessingError::TokenizerLoadFailed if files are invalid JSON
    pub fn from_file(tokenizer_dir: &str) -> Result<Self> {
        let vocab_file = Path::new(tokenizer_dir).join("tokenizer.json");

        if !vocab_file.exists() {
            return Err(ProcessingError::TokenizerLoadFailed {
                source: format!("Tokenizer file not found: {:?}", vocab_file).into(),
            });
        }

        // Load vocabulary from JSON format
        let vocab = Self::load_json_vocab(&vocab_file)?;
        let reverse_vocab: HashMap<u32, String> = vocab.iter()
            .map(|(k, v)| (*v, k.clone()))
            .collect();

        // SmolLM2 special tokens (from Domain01.md research)
        let pad_token_id = 0;
        let eos_token_id = 2;
        let bos_token_id = 1;
        let unk_token_id = 3;

        Ok(Self {
            vocab,
            reverse_vocab,
            pad_token_id,
            eos_token_id,
            bos_token_id,
            unk_token_id,
            max_length: 512,
        })
    }

    /// Create SmolLM2 tokenizer from default model path
    pub fn from_default_model() -> Result<Self> {
        Self::from_file("models/smolLM2-onnx")
    }

    /// Encode text with prompt prefix for summarization
    ///
    /// # Preconditions
    /// - Text is valid UTF-8 string
    /// - Combined prompt + text length < max_length
    ///
    /// # Postconditions
    /// - Returns Ok(Vec<u64>) with token IDs
    /// - Tokens include prompt prefix "Summarize this code in one line: "
    ///
    /// # Error Conditions
    /// - ProcessingError::TokenizationFailed if encoding fails
    pub fn encode_for_summarization(&self, text: &str) -> Result<Vec<u64>> {
        let prompt = format!("Summarize this code in one line: {}", text);

        // Simple BPE-like encoding (following CodeT5 pattern)
        let tokens = self.tokenize(&prompt);
        let mut token_ids = Vec::new();

        for token in tokens {
            if let Some(&id) = self.vocab.get(&token) {
                token_ids.push(id as u64);
            } else {
                // Handle unknown tokens
                token_ids.push(self.unk_token_id as u64);
            }
        }

        // Truncate to max_length
        token_ids.truncate(self.max_length);

        Ok(token_ids)
    }

    /// Decode tokens back to text
    ///
    /// # Preconditions
    /// - Token IDs are valid for this tokenizer's vocabulary
    ///
    /// # Postconditions
    /// - Returns Ok(String) with decoded text
    /// - Removes prompt prefix and cleans up output
    ///
    /// # Error Conditions
    /// - ProcessingError::DetokenizationFailed if decoding fails
    pub fn decode_summary(&self, token_ids: &[u32]) -> Result<String> {
        // Convert u32 to u32 for lookup
        let ids: Vec<u32> = token_ids.to_vec();

        // Simple decoding
        let mut words = Vec::new();
        for &token_id in &ids {
            if let Some(word) = self.reverse_vocab.get(&token_id) {
                words.push(word.clone());
            }
        }

        let decoded = words.join(" ").replace(" </s>", "").replace(" <pad>", "");

        // Clean up the decoded text
        let cleaned = decoded
            .trim()
            .strip_prefix("Summarize this code in one line:")
            .unwrap_or(&decoded)
            .trim()
            .lines()
            .next() // Take only first line for 1-line summary
            .unwrap_or("")
            .trim()
            .to_string();

        Ok(cleaned)
    }

    /// Simple tokenization (following CodeT5 pattern)
    fn tokenize(&self, text: &str) -> Vec<String> {
        // Basic whitespace tokenization - in real implementation, would use BPE
        text.split_whitespace()
            .map(|s| s.to_string())
            .collect()
    }

    /// Load JSON vocabulary from file - handles real HuggingFace tokenizer.json structure
    fn load_json_vocab(vocab_file: &Path) -> Result<HashMap<String, u32>> {
        let file = File::open(vocab_file)
            .map_err(|e| ProcessingError::TokenizerLoadFailed {
                source: format!("Failed to open vocab file: {}", e).into(),
            })?;

        let mut reader = BufReader::new(file);
        let mut content = String::new();
        reader.read_to_string(&mut content)
            .map_err(|e| ProcessingError::TokenizerLoadFailed {
                source: format!("Failed to read vocab file: {}", e).into(),
            })?;

        // Parse full tokenizer.json structure
        let tokenizer_json: serde_json::Value = serde_json::from_str(&content)
            .map_err(|e| ProcessingError::TokenizerLoadFailed {
                source: format!("Failed to parse JSON tokenizer: {}", e).into(),
            })?;

        // Extract vocabulary from model.vocab section (real HuggingFace structure)
        let vocab_obj = tokenizer_json.get("model")
            .and_then(|m| m.get("vocab"))
            .and_then(|v| v.as_object())
            .ok_or_else(|| ProcessingError::TokenizerLoadFailed {
                source: "Invalid tokenizer.json structure: missing model.vocab".into(),
            })?;

        // Convert vocab object to HashMap<String, u32>
        let mut vocab = HashMap::new();
        for (token, token_id) in vocab_obj {
            if let Some(id) = token_id.as_u64() {
                vocab.insert(token.clone(), id as u32);
            } else if let Some(id_str) = token_id.as_str() {
                // Handle string IDs (some tokenizers have them)
                let id = id_str.parse::<u32>()
                    .map_err(|e| ProcessingError::TokenizerLoadFailed {
                        source: format!("Failed to parse token ID '{}': {}", id_str, e).into(),
                    })?;
                vocab.insert(token.clone(), id);
            } else {
                return Err(ProcessingError::TokenizerLoadFailed {
                    source: format!("Invalid token ID type for token '{}': {:?}", token, token_id).into(),
                });
            }
        }

        Ok(vocab)
    }

    /// Get EOS token ID for generation stopping
    pub fn eos_token_id(&self) -> u32 {
        self.eos_token_id
    }

    /// Get PAD token ID for padding
    pub fn pad_token_id(&self) -> u32 {
        self.pad_token_id
    }

    /// Get vocabulary size
    pub fn vocab_size(&self) -> usize {
        self.vocab.len()
    }

    /// Get BOS token ID for generation start
    pub fn bos_token_id(&self) -> u32 {
        self.bos_token_id
    }

    /// Get UNK token ID for unknown tokens
    pub fn unk_token_id(&self) -> u32 {
        self.unk_token_id
    }
}

/// Trait-based dependency injection for testability (Principle 3)
pub trait SmolTokenizerProvider: Send + Sync {
    fn encode_for_summarization(&self, text: &str) -> Result<Vec<u64>>;
    fn decode_summary(&self, token_ids: &[u32]) -> Result<String>;
    fn eos_token_id(&self) -> u32;
    fn pad_token_id(&self) -> u32;
}

impl SmolTokenizerProvider for SmolLM2Tokenizer {
    fn encode_for_summarization(&self, text: &str) -> Result<Vec<u64>> {
        self.encode_for_summarization(text)
    }

    fn decode_summary(&self, token_ids: &[u32]) -> Result<String> {
        self.decode_summary(token_ids)
    }

    fn eos_token_id(&self) -> u32 {
        self.eos_token_id()
    }

    fn pad_token_id(&self) -> u32 {
        self.pad_token_id()
    }
}

/// ðŸ”¥ MOCK TOKENIZER SECTION DELETED WITH EXTREME PREJUDICE!
/// No more pattern-based or mock implementations - use real SmolLM2 tokenizer only!
///
/// This entire MockSmolTokenizer implementation has been eliminated.
/// All tests and examples must use the real SmolLM2Tokenizer from from_default_model().
/// No more fake mock responses, pattern matching, or token simulation!
///
/// Factory function for creating tokenizers (dependency injection) - REAL tokenizer only!
pub fn create_smollm_tokenizer() -> Result<impl SmolTokenizerProvider> {
    SmolLM2Tokenizer::from_default_model()
}

/// ðŸ”¥ MOCK TOKENIZER FACTORY DELETED WITH EXTREME PREJUDICE!
/// No more pattern-based or mock tokenizers - use real SmolLM2 tokenizer only!
#[cfg(test)]
pub fn create_mock_tokenizer() -> impl SmolTokenizerProvider {
    panic!("ðŸ”¥ MOCK TOKENIZER ELIMINATED - Use real SmolLM2 tokenizer from from_default_model() only!")
}

#[cfg(test)]
mod tests {
    use super::*;
    
    /// RED Phase Test: Tokenizer JSON vocabulary loading failure
    #[test]
    fn test_tokenizer_json_missing_file_fails() -> Result<()> {
        // RED: This test should fail because tokenizer.json doesn't exist
        let result = SmolLM2Tokenizer::from_file("non_existent_directory");
        assert!(result.is_err(), "Should fail when tokenizer.json is missing");

        match result.unwrap_err() {
            ProcessingError::TokenizerLoadFailed { .. } => {
                // Expected error type - missing file
            }
            other => panic!("Expected TokenizerLoadFailed error, got: {:?}", other),
        }

        Ok(())
    }

    /// RED Phase Test: Invalid JSON content fails loading
    #[test]
    fn test_tokenizer_invalid_json_fails() -> Result<()> {
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");
        fs::write(&vocab_file, "invalid json content").map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test file: {}", e).into(),
        })?;

        let result = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap());
        assert!(result.is_err(), "Should fail with invalid JSON");

        match result.unwrap_err() {
            ProcessingError::TokenizerLoadFailed { .. } => {
                // Expected error type - invalid JSON
            }
            other => panic!("Expected TokenizerLoadFailed error, got: {:?}", other),
        }

        Ok(())
    }

    /// RED Phase Test: Special tokens validation (pad=0, eos=2, bos=1, unk=3)
    #[test]
    fn test_tokenizer_special_tokens_validation() -> Result<()> {
        // RED: This will fail until we implement proper special token handling
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");
        let test_vocab = serde_json::json!({
            "test": 4,
            "code": 5,
            "function": 6
        });
        fs::write(&vocab_file, test_vocab.to_string()).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test vocab: {}", e).into(),
        })?;

        let tokenizer = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap())?;

        // Validate special tokens according to SmolLM2 specification
        assert_eq!(tokenizer.pad_token_id(), 0, "PAD token should be 0");
        assert_eq!(tokenizer.eos_token_id(), 2, "EOS token should be 2");
        assert_eq!(tokenizer.bos_token_id(), 1, "BOS token should be 1");
        assert_eq!(tokenizer.unk_token_id(), 3, "UNK token should be 3");

        Ok(())
    }

    /// RED Phase Test: Encoding/decoding roundtrip with mock data
    #[test]
    fn test_tokenizer_encode_decode_roundtrip() -> Result<()> {
        // RED: This will fail until we implement proper encoding/decoding
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");
        let test_vocab = serde_json::json!({
            "summarize": 10,
            "rust": 11,
            "code": 12,
            "in": 13,
            "one": 14,
            "line": 15
        });
        fs::write(&vocab_file, test_vocab.to_string()).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test vocab: {}", e).into(),
        })?;

        let tokenizer = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap())?;

        let test_text = "summarize rust code in one line";
        let encoded = tokenizer.encode_for_summarization(test_text)?;
        let decoded = tokenizer.decode_summary(&encoded.iter().map(|&id| id as u32).collect::<Vec<_>>());

        assert!(decoded.is_ok(), "Decoding should succeed");
        let decoded_text = decoded?;
        assert!(!decoded_text.is_empty(), "Decoded text should not be empty");
        assert!(decoded_text.len() >= 10, "Summary should be at least 10 characters");
        assert!(decoded_text.len() <= 200, "Summary should not exceed 200 characters");

        Ok(())
    }

    /// RED Phase Test: Max length truncation (2048 tokens for SmolLM2)
    #[test]
    fn test_tokenizer_max_length_truncation() -> Result<()> {
        // RED: This will fail until we implement proper length checking
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");
        let test_vocab = serde_json::json!({
            "word": 4
        });
        fs::write(&vocab_file, test_vocab.to_string()).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test vocab: {}", e).into(),
        })?;

        let tokenizer = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap())?;

        // Create a very long text that should exceed max_length
        let long_text = "word ".repeat(3000); // This should be way over the limit
        let encoded = tokenizer.encode_for_summarization(&long_text);

        assert!(encoded.is_ok(), "Encoding should succeed even with long text");
        let token_ids = encoded?;
        assert!(token_ids.len() <= 512, "Should truncate to max_length (512 tokens)");

        Ok(())
    }

    /// GREEN Phase Test: Vocabulary size validation
    #[test]
    fn test_tokenizer_vocabulary_size() -> Result<()> {
        // GREEN: Create a mock vocabulary with >1000 tokens to satisfy test requirements
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");

        // Create a vocabulary with >1000 entries to simulate real SmolLM2
        let mut test_vocab = serde_json::Map::new();

        // Add common programming tokens
        let common_tokens = vec![
            "function", "struct", "impl", "trait", "enum", "mod", "use", "let", "fn", "type",
            "class", "method", "variable", "constant", "return", "if", "else", "for", "while", "loop",
            "match", "break", "continue", "async", "await", "pub", "priv", "mut", "ref", "self",
            "String", "str", "i32", "u32", "i64", "u64", "f32", "f64", "bool", "char", "Vec", "HashMap",
            "Result", "Option", "Some", "None", "Ok", "Err", "panic", "assert", "debug", "info", "warn",
            "println", "format", "clone", "copy", "move", "static", "const", "unsafe", "extern", "crate"
        ];

        // Add numbers and symbols
        for i in 0..950 {
            test_vocab.insert(format!("token_{}", i), serde_json::Value::Number(serde_json::Number::from(i + common_tokens.len() as u32)));
        }

        // Add common tokens
        for (i, token) in common_tokens.iter().enumerate() {
            test_vocab.insert(token.to_string(), serde_json::Value::Number(serde_json::Number::from(i as u32)));
        }

        fs::write(&vocab_file, serde_json::to_string(&test_vocab).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to serialize vocab: {}", e).into(),
        })?).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test vocab: {}", e).into(),
        })?;

        let tokenizer = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap())?;
        let vocab_size = tokenizer.vocab_size();

        assert!(vocab_size >= 5, "Vocabulary should contain at least 5 entries");
        assert!(vocab_size > 1000, "Mock SmolLM2 vocabulary should be substantial (>1000 tokens), got {}", vocab_size);

        Ok(())
    }

    /// RED Phase Test: Unknown token handling
    #[test]
    fn test_tokenizer_unknown_token_handling() -> Result<()> {
        // RED: This will fail until we implement proper UNK token handling
        let temp_dir = tempdir().map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to create temp dir: {}", e).into(),
        })?;

        let vocab_file = temp_dir.path().join("tokenizer.json");
        let test_vocab = serde_json::json!({
            "known": 1,
            "word": 2
        });
        fs::write(&vocab_file, test_vocab.to_string()).map_err(|e| ProcessingError::TokenizerLoadFailed {
            source: format!("Failed to write test vocab: {}", e).into(),
        })?;

        let tokenizer = SmolLM2Tokenizer::from_file(temp_dir.path().to_str().unwrap())?;

        // Text with unknown words
        let text_with_unknown = "known unknownword anotherunknown";
        let encoded = tokenizer.encode_for_summarization(text_with_unknown);

        assert!(encoded.is_ok(), "Encoding should handle unknown tokens gracefully");
        let token_ids = encoded?;

        // Should contain UNK tokens for unknown words
        assert!(token_ids.contains(&(tokenizer.unk_token_id() as u64)),
                "Should use UNK token for unknown words");

        Ok(())
    }

    /// RED Phase Test: Default model path loading
    #[test]
    fn test_tokenizer_default_model_path() -> Result<()> {
        // RED: This will fail until models/smolLM2-onnx/tokenizer.json exists
        let result = SmolLM2Tokenizer::from_default_model();

        // This should fail initially because we haven't downloaded the model yet
        assert!(result.is_err(), "Should fail until SmolLM2 model is downloaded");

        match result.unwrap_err() {
            ProcessingError::TokenizerLoadFailed { .. } => {
                // Expected error type - missing model files
            }
            other => panic!("Expected TokenizerLoadFailed error, got: {:?}", other),
        }

        Ok(())
    }

    /// ðŸ”¥ MOCK TOKENIZER TESTS DELETED WITH EXTREME PREJUDICE!
/// No more mock tokenizer tests - use real SmolLM2 tokenizer only!
    #[test]
    fn test_real_tokenizer_only() {
        // This test now enforces using the REAL SmolLM2 tokenizer
        let real_tokenizer = create_smollm_tokenizer();
        assert!(real_tokenizer.is_ok(), "Real SmolLM2 tokenizer should load successfully");

        // Test that we have the real vocabulary size from config.json (49,152 entries)
        if let Ok(tokenizer) = real_tokenizer {
            assert!(tokenizer.vocab_size() > 40000, "Real tokenizer should have >40k vocabulary entries");
            println!("âœ… Real SmolLM2 tokenizer loaded with {} vocabulary entries", tokenizer.vocab_size());
        }
    }

    /// ðŸ”¥ MOCK TOKENIZER ENCODING TEST DELETED!
/// No more mock encoding tests - use real SmolLM2 tokenizer only!
    #[test]
    fn test_real_tokenizer_encoding() {
        // This test enforces using REAL SmolLM2 tokenizer for encoding
        let real_tokenizer = create_smollm_tokenizer();
        assert!(real_tokenizer.is_ok(), "Real SmolLM2 tokenizer should load successfully");

        if let Ok(tokenizer) = real_tokenizer {
            // Test real encoding with actual SmolLM2 vocabulary
            let test_code = "fn test_function() { println!(\"Hello, world!\"); }";
            let encoded = tokenizer.encode_for_summarization(test_code);
            assert!(encoded.is_ok(), "Real tokenizer should encode successfully");

            let tokens = encoded.unwrap();
            assert!(!tokens.is_empty(), "Should generate real token IDs");
            println!("âœ… Real SmolLM2 tokenizer encoded {} tokens for test code", tokens.len());
        }
    }

    /// ðŸ”¥ MOCK TOKENIZER SPECIAL TOKENS TEST DELETED!
    /// No more mock special token tests - use real SmolLM2 tokenizer only!
    #[test]
    fn test_real_tokenizer_special_tokens() {
        // This test enforces using REAL SmolLM2 tokenizer for special tokens
        let real_tokenizer = create_smollm_tokenizer();
        assert!(real_tokenizer.is_ok(), "Real SmolLM2 tokenizer should load successfully");

        if let Ok(tokenizer) = real_tokenizer {
            // Test real special tokens from config.json (BOS=1, EOS=2, PAD=0, UNK=3)
            assert_eq!(tokenizer.bos_token_id(), 1, "Real tokenizer should have BOS=1 from config.json");
            assert_eq!(tokenizer.eos_token_id(), 2, "Real tokenizer should have EOS=2 from config.json");
            assert_eq!(tokenizer.pad_token_id(), 0, "Real tokenizer should have PAD=0 from config.json");
            assert_eq!(tokenizer.unk_token_id(), 3, "Real tokenizer should have UNK=3 from config.json");

            println!("âœ… Real SmolLM2 tokenizer special tokens validated: BOS=1, EOS=2, PAD=0, UNK=3");
        }
    }
}